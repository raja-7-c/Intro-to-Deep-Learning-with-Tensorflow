{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "l10c03_nlp_constructing_text_generation_model.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "punL79CN7Ox6"
      },
      "source": [
        "##### Copyright 2020 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "_ckMIh7O7s6D"
      },
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ph5eir3Pf-3z"
      },
      "source": [
        "# Constructing a Text Generation Model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S5Uhzt6vVIB2"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/examples/blob/master/courses/udacity_intro_to_tensorflow_for_deep_learning/l10c03_nlp_constructing_text_generation_model.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/tensorflow/examples/blob/master/courses/udacity_intro_to_tensorflow_for_deep_learning/l10c03_nlp_constructing_text_generation_model.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7GbGfr_oLCat"
      },
      "source": [
        "Using most of the techniques you've already learned, it's now possible to generate new text by predicting the next word that follows a given seed word. To practice this method, we'll use the [Kaggle Song Lyrics Dataset](https://www.kaggle.com/mousehead/songlyrics)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4aHK2CYygXom"
      },
      "source": [
        "## Import TensorFlow and related functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2LmLTREBf5ng"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Other imports for processing data\n",
        "import string\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GmLTO_dpgge9"
      },
      "source": [
        "## Get the Dataset\n",
        "\n",
        "As noted above, we'll utilize the [Song Lyrics dataset](https://www.kaggle.com/mousehead/songlyrics) on Kaggle."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Bf5FVHfganK",
        "outputId": "dca94e2a-3072-4268-af42-70e822d33407",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!wget --no-check-certificate \\\n",
        "    https://drive.google.com/uc?id=1LiJFZd41ofrWoBtW-pMYsfz1w8Ny0Bj8 \\\n",
        "    -O /tmp/songdata.csv"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-07-01 05:00:16--  https://drive.google.com/uc?id=1LiJFZd41ofrWoBtW-pMYsfz1w8Ny0Bj8\n",
            "Resolving drive.google.com (drive.google.com)... 74.125.20.138, 74.125.20.102, 74.125.20.100, ...\n",
            "Connecting to drive.google.com (drive.google.com)|74.125.20.138|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-04-ak-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/14u07o3cvhff3eu2s2ubpqnps7ncl8ru/1625115600000/11118900490791463723/*/1LiJFZd41ofrWoBtW-pMYsfz1w8Ny0Bj8 [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2021-07-01 05:00:19--  https://doc-04-ak-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/14u07o3cvhff3eu2s2ubpqnps7ncl8ru/1625115600000/11118900490791463723/*/1LiJFZd41ofrWoBtW-pMYsfz1w8Ny0Bj8\n",
            "Resolving doc-04-ak-docs.googleusercontent.com (doc-04-ak-docs.googleusercontent.com)... 173.194.202.132, 2607:f8b0:400e:c00::84\n",
            "Connecting to doc-04-ak-docs.googleusercontent.com (doc-04-ak-docs.googleusercontent.com)|173.194.202.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/csv]\n",
            "Saving to: ‘/tmp/songdata.csv’\n",
            "\n",
            "/tmp/songdata.csv       [   <=>              ]  69.08M   114MB/s    in 0.6s    \n",
            "\n",
            "2021-07-01 05:00:20 (114 MB/s) - ‘/tmp/songdata.csv’ saved [72436445]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gu1BTzMIS1oy"
      },
      "source": [
        "## **First 10 Songs**\n",
        "\n",
        "Let's first look at just 10 songs from the dataset, and see how things perform."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fmb9rGaAUDO-"
      },
      "source": [
        "### Preprocessing\n",
        "\n",
        "Let's perform some basic preprocessing to get rid of punctuation and make everything lowercase. We'll then split the lyrics up by line and tokenize the lyrics."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2AVAvyF_Vuh5"
      },
      "source": [
        "def tokenize_corpus(corpus, num_words=-1):\n",
        "  # Fit a Tokenizer on the corpus\n",
        "  if num_words > -1:\n",
        "    tokenizer = Tokenizer(num_words=num_words)\n",
        "  else:\n",
        "    tokenizer = Tokenizer()\n",
        "  tokenizer.fit_on_texts(corpus)\n",
        "  return tokenizer\n",
        "\n",
        "def create_lyrics_corpus(dataset, field):\n",
        "  # Remove all other punctuation\n",
        "  dataset[field] = dataset[field].str.replace('[{}]'.format(string.punctuation), '')\n",
        "  # Make it lowercase\n",
        "  dataset[field] = dataset[field].str.lower()\n",
        "  # Make it one long string to split by line\n",
        "  lyrics = dataset[field].str.cat()\n",
        "  corpus = lyrics.split('\\n')\n",
        "  # Remove any trailing whitespace\n",
        "  for l in range(len(corpus)):\n",
        "    corpus[l] = corpus[l].rstrip()\n",
        "  # Remove any empty lines\n",
        "  corpus = [l for l in corpus if l != '']\n",
        "\n",
        "  return corpus"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "apcEXp7WhVBs",
        "outputId": "49429c8e-8aed-40b7-f216-daea61d73082",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Read the dataset from csv - just first 10 songs for now\n",
        "dataset = pd.read_csv('/tmp/songdata.csv', dtype=str)[:10]\n",
        "# Create the corpus using the 'text' column containing lyrics\n",
        "corpus = create_lyrics_corpus(dataset, 'text')\n",
        "# Tokenize the corpus\n",
        "tokenizer = tokenize_corpus(corpus)\n",
        "\n",
        "total_words = len(tokenizer.word_index) + 1\n",
        "\n",
        "print(tokenizer.word_index)\n",
        "print(total_words)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'you': 1, 'i': 2, 'and': 3, 'a': 4, 'me': 5, 'the': 6, 'is': 7, 'my': 8, 'to': 9, 'ma': 10, 'it': 11, 'of': 12, 'im': 13, 'your': 14, 'love': 15, 'so': 16, 'as': 17, 'that': 18, 'in': 19, 'andante': 20, 'boomaboomerang': 21, 'make': 22, 'on': 23, 'oh': 24, 'for': 25, 'but': 26, 'new': 27, 'bang': 28, 'its': 29, 'be': 30, 'like': 31, 'know': 32, 'now': 33, 'how': 34, 'could': 35, 'youre': 36, 'sing': 37, 'never': 38, 'no': 39, 'chiquitita': 40, 'can': 41, 'we': 42, 'song': 43, 'had': 44, 'good': 45, 'youll': 46, 'she': 47, 'just': 48, 'girl': 49, 'again': 50, 'will': 51, 'take': 52, 'please': 53, 'let': 54, 'am': 55, 'eyes': 56, 'was': 57, 'always': 58, 'cassandra': 59, 'blue': 60, 'time': 61, 'dont': 62, 'were': 63, 'return': 64, 'once': 65, 'then': 66, 'sorry': 67, 'cryin': 68, 'over': 69, 'feel': 70, 'ever': 71, 'believe': 72, 'what': 73, 'do': 74, 'go': 75, 'all': 76, 'out': 77, 'think': 78, 'every': 79, 'leave': 80, 'look': 81, 'at': 82, 'way': 83, 'one': 84, 'music': 85, 'down': 86, 'our': 87, 'give': 88, 'learn': 89, 'more': 90, 'us': 91, 'would': 92, 'there': 93, 'before': 94, 'when': 95, 'with': 96, 'feeling': 97, 'play': 98, 'cause': 99, 'away': 100, 'here': 101, 'have': 102, 'yes': 103, 'baby': 104, 'get': 105, 'didnt': 106, 'see': 107, 'did': 108, 'closed': 109, 'realized': 110, 'crazy': 111, 'world': 112, 'lord': 113, 'shes': 114, 'kind': 115, 'without': 116, 'if': 117, 'touch': 118, 'strong': 119, 'making': 120, 'such': 121, 'found': 122, 'true': 123, 'stay': 124, 'together': 125, 'thought': 126, 'come': 127, 'they': 128, 'sweet': 129, 'tender': 130, 'sender': 131, 'tune': 132, 'humdehumhum': 133, 'gonna': 134, 'last': 135, 'leaving': 136, 'sleep': 137, 'only': 138, 'saw': 139, 'tell': 140, 'hes': 141, 'her': 142, 'sound': 143, 'tread': 144, 'lightly': 145, 'ground': 146, 'ill': 147, 'show': 148, 'life': 149, 'too': 150, 'used': 151, 'darling': 152, 'meant': 153, 'break': 154, 'end': 155, 'yourself': 156, 'little': 157, 'dumbedumdum': 158, 'bedumbedumdum': 159, 'youve': 160, 'dumbbedumbdumb': 161, 'bedumbbedumbdumb': 162, 'by': 163, 'theyre': 164, 'alone': 165, 'misunderstood': 166, 'day': 167, 'dawning': 168, 'some': 169, 'wanted': 170, 'none': 171, 'listen': 172, 'words': 173, 'warning': 174, 'darkest': 175, 'nights': 176, 'nobody': 177, 'knew': 178, 'fight': 179, 'caught': 180, 'really': 181, 'power': 182, 'dreams': 183, 'weave': 184, 'until': 185, 'final': 186, 'hour': 187, 'morning': 188, 'ship': 189, 'gone': 190, 'grieving': 191, 'still': 192, 'pain': 193, 'cry': 194, 'sun': 195, 'try': 196, 'face': 197, 'something': 198, 'sees': 199, 'makes': 200, 'fine': 201, 'who': 202, 'mine': 203, 'leaves': 204, 'walk': 205, 'hand': 206, 'well': 207, 'about': 208, 'things': 209, 'slow': 210, 'theres': 211, 'talk': 212, 'why': 213, 'up': 214, 'lousy': 215, 'packing': 216, 'ive': 217, 'gotta': 218, 'near': 219, 'keeping': 220, 'intention': 221, 'growing': 222, 'taking': 223, 'dimension': 224, 'even': 225, 'better': 226, 'thank': 227, 'god': 228, 'not': 229, 'somebody': 230, 'happy': 231, 'question': 232, 'smile': 233, 'mean': 234, 'much': 235, 'kisses': 236, 'around': 237, 'anywhere': 238, 'advice': 239, 'care': 240, 'use': 241, 'selfish': 242, 'tool': 243, 'fool': 244, 'showing': 245, 'boomerang': 246, 'throwing': 247, 'warm': 248, 'kiss': 249, 'surrender': 250, 'giving': 251, 'been': 252, 'door': 253, 'burning': 254, 'bridges': 255, 'being': 256, 'moving': 257, 'though': 258, 'behind': 259, 'are': 260, 'must': 261, 'sure': 262, 'stood': 263, 'hope': 264, 'this': 265, 'deny': 266, 'sad': 267, 'quiet': 268, 'truth': 269, 'heartaches': 270, 'scars': 271, 'dancing': 272, 'sky': 273, 'shining': 274, 'above': 275, 'hear': 276, 'came': 277, 'couldnt': 278, 'everything': 279, 'back': 280, 'long': 281, 'waitin': 282, 'cold': 283, 'chills': 284, 'bone': 285, 'youd': 286, 'wonderful': 287, 'means': 288, 'special': 289, 'smiles': 290, 'lucky': 291, 'fellow': 292, 'park': 293, 'holds': 294, 'squeezes': 295, 'walking': 296, 'hours': 297, 'talking': 298, 'plan': 299, 'easy': 300, 'gently': 301, 'summer': 302, 'evening': 303, 'breeze': 304, 'grow': 305, 'fingers': 306, 'soft': 307, 'light': 308, 'body': 309, 'velvet': 310, 'night': 311, 'soul': 312, 'slowly': 313, 'shimmer': 314, 'thousand': 315, 'butterflies': 316, 'float': 317, 'put': 318, 'rotten': 319, 'boy': 320, 'tough': 321, 'stuff': 322, 'saying': 323, 'need': 324, 'anymore': 325, 'enough': 326, 'standing': 327, 'creep': 328, 'felt': 329, 'cheap': 330, 'notion': 331, 'deep': 332, 'dumb': 333, 'mistake': 334, 'entitled': 335, 'another': 336, 'beg': 337, 'forgive': 338, 'an': 339, 'feels': 340, 'hoot': 341, 'holler': 342, 'mad': 343, 'under': 344, 'heel': 345, 'holy': 346, 'christ': 347, 'deal': 348, 'sick': 349, 'tired': 350, 'tedious': 351, 'ways': 352, 'aint': 353, 'walkin': 354, 'cutting': 355, 'tie': 356, 'wanna': 357, 'into': 358, 'eye': 359, 'myself': 360, 'counting': 361, 'pride': 362, 'unright': 363, 'neighbours': 364, 'ride': 365, 'burying': 366, 'past': 367, 'peace': 368, 'free': 369, 'sucker': 370, 'street': 371, 'singing': 372, 'shouting': 373, 'staying': 374, 'alive': 375, 'city': 376, 'dead': 377, 'hiding': 378, 'their': 379, 'shame': 380, 'hollow': 381, 'laughter': 382, 'while': 383, 'crying': 384, 'bed': 385, 'pity': 386, 'believed': 387, 'lost': 388, 'from': 389, 'start': 390, 'suffer': 391, 'sell': 392, 'secrets': 393, 'bargain': 394, 'playing': 395, 'smart': 396, 'aching': 397, 'hearts': 398, 'sailing': 399, 'father': 400, 'sister': 401, 'reason': 402, 'linger': 403, 'deeply': 404, 'future': 405, 'casting': 406, 'shadow': 407, 'else': 408, 'fate': 409, 'bags': 410, 'thorough': 411, 'knowing': 412, 'late': 413, 'wait': 414, 'watched': 415, 'harbor': 416, 'sunrise': 417, 'sails': 418, 'almost': 419, 'slack': 420, 'cool': 421, 'rain': 422, 'deck': 423, 'tiny': 424, 'figure': 425, 'rigid': 426, 'restrained': 427, 'filled': 428, 'whats': 429, 'wrong': 430, 'enchained': 431, 'own': 432, 'sorrow': 433, 'tomorrow': 434, 'hate': 435, 'shoulder': 436, 'best': 437, 'friend': 438, 'rely': 439, 'broken': 440, 'feather': 441, 'patch': 442, 'walls': 443, 'tumbling': 444, 'loves': 445, 'blown': 446, 'candle': 447, 'seems': 448, 'hard': 449, 'handle': 450, 'id': 451, 'thinking': 452, 'went': 453, 'house': 454, 'hardly': 455, 'guy': 456, 'closing': 457, 'front': 458, 'emptiness': 459, 'he': 460, 'disapeared': 461, 'his': 462, 'car': 463, 'stunned': 464, 'dreamed': 465, 'lifes': 466, 'part': 467, 'move': 468, 'feet': 469, 'pavement': 470, 'acted': 471, 'told': 472, 'lies': 473, 'meet': 474, 'other': 475, 'guys': 476, 'stupid': 477, 'blind': 478, 'smiled': 479, 'took': 480, 'said': 481, 'may': 482, 'couple': 483, 'men': 484, 'them': 485, 'brother': 486, 'joe': 487, 'seeing': 488, 'lot': 489, 'him': 490, 'nice': 491, 'sitting': 492, 'sittin': 493, 'memories': 494}\n",
            "495\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v9x68iN_X6FK"
      },
      "source": [
        "### Create Sequences and Labels\n",
        "\n",
        "After preprocessing, we next need to create sequences and labels. Creating the sequences themselves is similar to before with `texts_to_sequences`, but also including the use of [N-Grams](https://towardsdatascience.com/introduction-to-language-models-n-gram-e323081503d9); creating the labels will now utilize those sequences as well as utilize one-hot encoding over all potential output words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QmlTsUqfikVO"
      },
      "source": [
        "sequences = []\n",
        "for line in corpus:\n",
        "\ttoken_list = tokenizer.texts_to_sequences([line])[0]\n",
        "\tfor i in range(1, len(token_list)):\n",
        "\t\tn_gram_sequence = token_list[:i+1]\n",
        "\t\tsequences.append(n_gram_sequence)\n",
        "\n",
        "# Pad sequences for equal input length \n",
        "max_sequence_len = max([len(seq) for seq in sequences])\n",
        "sequences = np.array(pad_sequences(sequences, maxlen=max_sequence_len, padding='pre'))\n",
        "\n",
        "# Split sequences between the \"input\" sequence and \"output\" predicted word\n",
        "input_sequences, labels = sequences[:,:-1], sequences[:,-1]\n",
        "# One-hot encode the labels\n",
        "one_hot_labels = tf.keras.utils.to_categorical(labels, num_classes=total_words)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zsmu3aEId49i",
        "outputId": "229514be-062b-413a-b54a-05161d55ba12",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Check out how some of our data is being stored\n",
        "# The Tokenizer has just a single index per word\n",
        "print(tokenizer.word_index['know'])\n",
        "print(tokenizer.word_index['feeling'])\n",
        "# Input sequences will have multiple indexes\n",
        "print(input_sequences[5])\n",
        "print(input_sequences[6])\n",
        "# And the one hot labels will be as long as the full spread of tokenized words\n",
        "print(one_hot_labels[5])\n",
        "print(one_hot_labels[6])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "32\n",
            "97\n",
            "[  0   0   0   0   0   0   0   0   0   0   0   0   0  81  82 142 197  29\n",
            "   4]\n",
            "[  0   0   0   0   0   0   0   0   0   0   0   0  81  82 142 197  29   4\n",
            " 287]\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-1TAJMlmfO8r"
      },
      "source": [
        "### Train a Text Generation Model\n",
        "\n",
        "Building an RNN to train our text generation model will be very similar to the sentiment models you've built previously. The only real change necessary is to make sure to use Categorical instead of Binary Cross Entropy as the loss function - we could use Binary before since the sentiment was only 0 or 1, but now there are hundreds of categories.\n",
        "\n",
        "From there, we should also consider using *more* epochs than before, as text generation can take a little longer to converge than sentiment analysis, *and* we aren't working with all that much data yet. I'll set it at 200 epochs here since we're only use part of the dataset, and training will tail off quite a bit over that many epochs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G1YXuxIqfygN",
        "outputId": "058f14de-7c5c-4074-afd8-32441801c53d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(total_words, 64, input_length=max_sequence_len-1))\n",
        "model.add(Bidirectional(LSTM(20)))\n",
        "model.add(Dense(total_words, activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "history = model.fit(input_sequences, one_hot_labels, epochs=200, verbose=1)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "62/62 [==============================] - 9s 7ms/step - loss: 6.0221 - accuracy: 0.0242\n",
            "Epoch 2/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 5.4517 - accuracy: 0.0272\n",
            "Epoch 3/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 5.3692 - accuracy: 0.0353\n",
            "Epoch 4/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 5.3138 - accuracy: 0.0404\n",
            "Epoch 5/200\n",
            "62/62 [==============================] - 0s 6ms/step - loss: 5.2451 - accuracy: 0.0409\n",
            "Epoch 6/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 5.1772 - accuracy: 0.0394\n",
            "Epoch 7/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 5.1063 - accuracy: 0.0424\n",
            "Epoch 8/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 5.0274 - accuracy: 0.0434\n",
            "Epoch 9/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 4.9496 - accuracy: 0.0494\n",
            "Epoch 10/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 4.8583 - accuracy: 0.0636\n",
            "Epoch 11/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 4.7658 - accuracy: 0.0863\n",
            "Epoch 12/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 4.6686 - accuracy: 0.0918\n",
            "Epoch 13/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 4.5687 - accuracy: 0.1125\n",
            "Epoch 14/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 4.4609 - accuracy: 0.1211\n",
            "Epoch 15/200\n",
            "62/62 [==============================] - 0s 6ms/step - loss: 4.3683 - accuracy: 0.1297\n",
            "Epoch 16/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 4.2606 - accuracy: 0.1433\n",
            "Epoch 17/200\n",
            "62/62 [==============================] - 0s 6ms/step - loss: 4.1661 - accuracy: 0.1630\n",
            "Epoch 18/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 4.0835 - accuracy: 0.1731\n",
            "Epoch 19/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 3.9905 - accuracy: 0.1937\n",
            "Epoch 20/200\n",
            "62/62 [==============================] - 0s 6ms/step - loss: 3.9132 - accuracy: 0.2129\n",
            "Epoch 21/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 3.8317 - accuracy: 0.2296\n",
            "Epoch 22/200\n",
            "62/62 [==============================] - 0s 6ms/step - loss: 3.7472 - accuracy: 0.2568\n",
            "Epoch 23/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 3.6687 - accuracy: 0.2664\n",
            "Epoch 24/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 3.5960 - accuracy: 0.2765\n",
            "Epoch 25/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 3.5336 - accuracy: 0.2911\n",
            "Epoch 26/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 3.4578 - accuracy: 0.3169\n",
            "Epoch 27/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 3.3819 - accuracy: 0.3330\n",
            "Epoch 28/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 3.3211 - accuracy: 0.3350\n",
            "Epoch 29/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 3.2571 - accuracy: 0.3577\n",
            "Epoch 30/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 3.2234 - accuracy: 0.3618\n",
            "Epoch 31/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 3.1720 - accuracy: 0.3663\n",
            "Epoch 32/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 3.1113 - accuracy: 0.3729\n",
            "Epoch 33/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 3.0268 - accuracy: 0.3885\n",
            "Epoch 34/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.9632 - accuracy: 0.4006\n",
            "Epoch 35/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.9003 - accuracy: 0.4137\n",
            "Epoch 36/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.8499 - accuracy: 0.4268\n",
            "Epoch 37/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.8127 - accuracy: 0.4284\n",
            "Epoch 38/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.7696 - accuracy: 0.4299\n",
            "Epoch 39/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.7074 - accuracy: 0.4420\n",
            "Epoch 40/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.6615 - accuracy: 0.4495\n",
            "Epoch 41/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.6002 - accuracy: 0.4591\n",
            "Epoch 42/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.5473 - accuracy: 0.4687\n",
            "Epoch 43/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.5265 - accuracy: 0.4733\n",
            "Epoch 44/200\n",
            "62/62 [==============================] - 0s 6ms/step - loss: 2.4666 - accuracy: 0.4813\n",
            "Epoch 45/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.4258 - accuracy: 0.4884\n",
            "Epoch 46/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.3743 - accuracy: 0.5020\n",
            "Epoch 47/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.3377 - accuracy: 0.5106\n",
            "Epoch 48/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.2916 - accuracy: 0.5202\n",
            "Epoch 49/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.2720 - accuracy: 0.5227\n",
            "Epoch 50/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.2362 - accuracy: 0.5252\n",
            "Epoch 51/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.1928 - accuracy: 0.5348\n",
            "Epoch 52/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.1461 - accuracy: 0.5429\n",
            "Epoch 53/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.1066 - accuracy: 0.5484\n",
            "Epoch 54/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.0653 - accuracy: 0.5616\n",
            "Epoch 55/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.0312 - accuracy: 0.5772\n",
            "Epoch 56/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.0107 - accuracy: 0.5767\n",
            "Epoch 57/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.9744 - accuracy: 0.5873\n",
            "Epoch 58/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.9471 - accuracy: 0.5858\n",
            "Epoch 59/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.9366 - accuracy: 0.5908\n",
            "Epoch 60/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.9107 - accuracy: 0.5943\n",
            "Epoch 61/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.8881 - accuracy: 0.5974\n",
            "Epoch 62/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.8533 - accuracy: 0.6054\n",
            "Epoch 63/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.8218 - accuracy: 0.6186\n",
            "Epoch 64/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.7910 - accuracy: 0.6221\n",
            "Epoch 65/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.7565 - accuracy: 0.6413\n",
            "Epoch 66/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.7128 - accuracy: 0.6514\n",
            "Epoch 67/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.6864 - accuracy: 0.6534\n",
            "Epoch 68/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.6734 - accuracy: 0.6549\n",
            "Epoch 69/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.6468 - accuracy: 0.6604\n",
            "Epoch 70/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.6145 - accuracy: 0.6705\n",
            "Epoch 71/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.6212 - accuracy: 0.6594\n",
            "Epoch 72/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.6041 - accuracy: 0.6680\n",
            "Epoch 73/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.5601 - accuracy: 0.6705\n",
            "Epoch 74/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.5406 - accuracy: 0.6826\n",
            "Epoch 75/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.5207 - accuracy: 0.6862\n",
            "Epoch 76/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.4971 - accuracy: 0.6882\n",
            "Epoch 77/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.4683 - accuracy: 0.6932\n",
            "Epoch 78/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.4448 - accuracy: 0.7059\n",
            "Epoch 79/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.4972 - accuracy: 0.6761\n",
            "Epoch 80/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.4193 - accuracy: 0.7109\n",
            "Epoch 81/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.3866 - accuracy: 0.7175\n",
            "Epoch 82/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.3869 - accuracy: 0.7114\n",
            "Epoch 83/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.3469 - accuracy: 0.7240\n",
            "Epoch 84/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.3185 - accuracy: 0.7275\n",
            "Epoch 85/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.2905 - accuracy: 0.7341\n",
            "Epoch 86/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.2831 - accuracy: 0.7331\n",
            "Epoch 87/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.2677 - accuracy: 0.7412\n",
            "Epoch 88/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.2513 - accuracy: 0.7331\n",
            "Epoch 89/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.2376 - accuracy: 0.7341\n",
            "Epoch 90/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.2162 - accuracy: 0.7442\n",
            "Epoch 91/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.2051 - accuracy: 0.7462\n",
            "Epoch 92/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.1938 - accuracy: 0.7452\n",
            "Epoch 93/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.1692 - accuracy: 0.7543\n",
            "Epoch 94/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.1492 - accuracy: 0.7533\n",
            "Epoch 95/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.1330 - accuracy: 0.7608\n",
            "Epoch 96/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.1126 - accuracy: 0.7649\n",
            "Epoch 97/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.0965 - accuracy: 0.7689\n",
            "Epoch 98/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.0797 - accuracy: 0.7694\n",
            "Epoch 99/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.0660 - accuracy: 0.7719\n",
            "Epoch 100/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.0525 - accuracy: 0.7765\n",
            "Epoch 101/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.0400 - accuracy: 0.7770\n",
            "Epoch 102/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.0326 - accuracy: 0.7785\n",
            "Epoch 103/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.0250 - accuracy: 0.7765\n",
            "Epoch 104/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.0129 - accuracy: 0.7815\n",
            "Epoch 105/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.0111 - accuracy: 0.7851\n",
            "Epoch 106/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.9910 - accuracy: 0.7866\n",
            "Epoch 107/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.9678 - accuracy: 0.7896\n",
            "Epoch 108/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.9539 - accuracy: 0.7916\n",
            "Epoch 109/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.9393 - accuracy: 0.7957\n",
            "Epoch 110/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.9274 - accuracy: 0.7947\n",
            "Epoch 111/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.9183 - accuracy: 0.7967\n",
            "Epoch 112/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.9144 - accuracy: 0.7967\n",
            "Epoch 113/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.9263 - accuracy: 0.7891\n",
            "Epoch 114/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.8964 - accuracy: 0.7987\n",
            "Epoch 115/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.8876 - accuracy: 0.8063\n",
            "Epoch 116/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.8782 - accuracy: 0.8047\n",
            "Epoch 117/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.8613 - accuracy: 0.8103\n",
            "Epoch 118/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.8540 - accuracy: 0.8103\n",
            "Epoch 119/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.8401 - accuracy: 0.8133\n",
            "Epoch 120/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.8359 - accuracy: 0.8143\n",
            "Epoch 121/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.8258 - accuracy: 0.8169\n",
            "Epoch 122/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.8088 - accuracy: 0.8229\n",
            "Epoch 123/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.7973 - accuracy: 0.8239\n",
            "Epoch 124/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.7909 - accuracy: 0.8285\n",
            "Epoch 125/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.7997 - accuracy: 0.8148\n",
            "Epoch 126/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.7917 - accuracy: 0.8269\n",
            "Epoch 127/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.7777 - accuracy: 0.8320\n",
            "Epoch 128/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.7693 - accuracy: 0.8310\n",
            "Epoch 129/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.8215 - accuracy: 0.8108\n",
            "Epoch 130/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.7839 - accuracy: 0.8315\n",
            "Epoch 131/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.7611 - accuracy: 0.8345\n",
            "Epoch 132/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.7420 - accuracy: 0.8426\n",
            "Epoch 133/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.7253 - accuracy: 0.8471\n",
            "Epoch 134/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.7123 - accuracy: 0.8436\n",
            "Epoch 135/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.7032 - accuracy: 0.8481\n",
            "Epoch 136/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.7023 - accuracy: 0.8436\n",
            "Epoch 137/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.6931 - accuracy: 0.8507\n",
            "Epoch 138/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.6892 - accuracy: 0.8496\n",
            "Epoch 139/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.6852 - accuracy: 0.8507\n",
            "Epoch 140/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.6729 - accuracy: 0.8507\n",
            "Epoch 141/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.6752 - accuracy: 0.8512\n",
            "Epoch 142/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.6718 - accuracy: 0.8512\n",
            "Epoch 143/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.6563 - accuracy: 0.8572\n",
            "Epoch 144/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.6507 - accuracy: 0.8582\n",
            "Epoch 145/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.6521 - accuracy: 0.8532\n",
            "Epoch 146/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.6578 - accuracy: 0.8552\n",
            "Epoch 147/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.6419 - accuracy: 0.8592\n",
            "Epoch 148/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.6318 - accuracy: 0.8628\n",
            "Epoch 149/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.6194 - accuracy: 0.8618\n",
            "Epoch 150/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.6112 - accuracy: 0.8648\n",
            "Epoch 151/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.6047 - accuracy: 0.8623\n",
            "Epoch 152/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.6010 - accuracy: 0.8633\n",
            "Epoch 153/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5968 - accuracy: 0.8623\n",
            "Epoch 154/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.6273 - accuracy: 0.8552\n",
            "Epoch 155/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.6443 - accuracy: 0.8486\n",
            "Epoch 156/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.6206 - accuracy: 0.8577\n",
            "Epoch 157/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5931 - accuracy: 0.8668\n",
            "Epoch 158/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5857 - accuracy: 0.8658\n",
            "Epoch 159/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5798 - accuracy: 0.8638\n",
            "Epoch 160/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5614 - accuracy: 0.8713\n",
            "Epoch 161/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5555 - accuracy: 0.8739\n",
            "Epoch 162/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5529 - accuracy: 0.8703\n",
            "Epoch 163/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5435 - accuracy: 0.8734\n",
            "Epoch 164/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5373 - accuracy: 0.8744\n",
            "Epoch 165/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5345 - accuracy: 0.8804\n",
            "Epoch 166/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5444 - accuracy: 0.8688\n",
            "Epoch 167/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5852 - accuracy: 0.8633\n",
            "Epoch 168/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5532 - accuracy: 0.8683\n",
            "Epoch 169/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5462 - accuracy: 0.8703\n",
            "Epoch 170/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5264 - accuracy: 0.8744\n",
            "Epoch 171/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5176 - accuracy: 0.8769\n",
            "Epoch 172/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5091 - accuracy: 0.8819\n",
            "Epoch 173/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5029 - accuracy: 0.8809\n",
            "Epoch 174/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5051 - accuracy: 0.8794\n",
            "Epoch 175/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5452 - accuracy: 0.8648\n",
            "Epoch 176/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5353 - accuracy: 0.8718\n",
            "Epoch 177/200\n",
            "62/62 [==============================] - 0s 6ms/step - loss: 0.5223 - accuracy: 0.8764\n",
            "Epoch 178/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5104 - accuracy: 0.8789\n",
            "Epoch 179/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.4967 - accuracy: 0.8819\n",
            "Epoch 180/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.4884 - accuracy: 0.8819\n",
            "Epoch 181/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.4817 - accuracy: 0.8814\n",
            "Epoch 182/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.4760 - accuracy: 0.8814\n",
            "Epoch 183/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.4699 - accuracy: 0.8850\n",
            "Epoch 184/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.4685 - accuracy: 0.8845\n",
            "Epoch 185/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.4608 - accuracy: 0.8835\n",
            "Epoch 186/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.4590 - accuracy: 0.8870\n",
            "Epoch 187/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.4540 - accuracy: 0.8860\n",
            "Epoch 188/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.4505 - accuracy: 0.8860\n",
            "Epoch 189/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.4505 - accuracy: 0.8860\n",
            "Epoch 190/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.4441 - accuracy: 0.8890\n",
            "Epoch 191/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.4400 - accuracy: 0.8890\n",
            "Epoch 192/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.4420 - accuracy: 0.8895\n",
            "Epoch 193/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.4810 - accuracy: 0.8718\n",
            "Epoch 194/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.4448 - accuracy: 0.8860\n",
            "Epoch 195/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.4407 - accuracy: 0.8865\n",
            "Epoch 196/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.4346 - accuracy: 0.8855\n",
            "Epoch 197/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.4249 - accuracy: 0.8905\n",
            "Epoch 198/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.4180 - accuracy: 0.8925\n",
            "Epoch 199/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.4166 - accuracy: 0.8900\n",
            "Epoch 200/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.4120 - accuracy: 0.8925\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXVFpoREhV6Y"
      },
      "source": [
        "### View the Training Graph"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aeSNfS7uhch0",
        "outputId": "58516809-02bf-40a6-abf5-c1f6011621f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_graphs(history, string):\n",
        "  plt.plot(history.history[string])\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(string)\n",
        "  plt.show()\n",
        "\n",
        "plot_graphs(history, 'accuracy')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1rAgRpxYhjpB"
      },
      "source": [
        "### Generate new lyrics!\n",
        "\n",
        "It's finally time to generate some new lyrics from the trained model, and see what we get. To do so, we'll provide some \"seed text\", or an input sequence for the model to start with. We'll also decide just how long of an output sequence we want - this could essentially be infinite, as the input plus the previous output will be continuously fed in for a new output word (at least up to our max sequence length)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DC7zfcgviDTp",
        "outputId": "6f28ea60-3a93-49fc-af51-94124f9221f5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "seed_text = \"im feeling chills\"\n",
        "next_words = 100\n",
        "  \n",
        "for _ in range(next_words):\n",
        "\ttoken_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "\ttoken_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
        "\tpredicted = np.argmax(model.predict(token_list), axis=-1)\n",
        "\toutput_word = \"\"\n",
        "\tfor word, index in tokenizer.word_index.items():\n",
        "\t\tif index == predicted:\n",
        "\t\t\toutput_word = word\n",
        "\t\t\tbreak\n",
        "\tseed_text += \" \" + output_word\n",
        "print(seed_text)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "im feeling chills it as dreams you would weave free to handle end the scars feet shame shame heel heel scars eye scars scars scars us scars scars scars scars scars sky music heartaches bone truth truth heartaches sky truth truth bags scars touch bed heel heel bone never scars park park bone music words of song song dont truth truth bags such new had over over for you baby saw for you holler and you my stuff shame shame brother heel stuff shame stuff heel heel heel didnt heel ground for for see making and day in our pain me took had\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}